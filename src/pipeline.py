import logging
import json
from typing import List
import pandas as pd
import re
from langchain.chat_models import init_chat_model
from langchain.callbacks.tracers import LangChainTracer

from utils import dump_in_jsonl



logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class LLM:
    """Base Language Model class for handling LLM interactions.

    This class provides a foundation for working with Language Models, handling model initialization,
    inference calls, and error management. It includes functionality for tracing model interactions
    using LangChain and structured logging.

    Attributes:
        model (str): The identifier for the language model to be used.

    Methods:
        call_llm: Executes model inference with error handling and logging.
    """
    def __init__(self,model:str):
        self.model = model

    def call_llm(self,text:str,intention:str,temperature=0.0,tracer_project:str=None,**kwargs):
        logger.debug(f"Calling : {self.model} | Intention : {intention} ...") 
        try:
            response = init_chat_model(model=self.model,temperature=temperature)
            response = response.invoke(
                text,
                config= {"callbacks": [LangChainTracer(project_name=tracer_project)]} if tracer_project else {}
                ).content
            logger.info(f"Successfully called : {self.model} | Intention : {intention}.")
            result = {
                    f'{intention} model':self.model,
                    f'Encountered Problems':False,
                    f'{intention} model response':response
                }
        except Exception as e:  
            logger.error(f"Failed to call : {self.model} | Intention : {intention} | Error : {e}")
            result = {
                    f'{intention} model':self.model,
                    f'Encountered Problems':True,
                    f'{intention} model response':str(e)
                }
            
        kwargs.update(result)
        return kwargs

    

class JudgeLLM(LLM):
    """Judge Language Model class for evaluating model responses.

    This class extends the base LLM class to provide functionality for evaluating
    responses from language models. It manages multiple judge models that can assess
    the quality, relevance, and accuracy of model-generated content.

    Attributes:
        judges (List[LLM]): A list of LLM instances that serve as judges for evaluation.

    Methods:
        calling_judges: Coordinates the evaluation of model responses using the judge models.
        fetch_evaluation_prompt: Retrieves the appropriate evaluation prompt template.
        fill_out_prompt: Populates the evaluation prompt template with specific content.
    """
    def __init__(self,judges:List[str]):
        self.judges = [LLM(i) for i in judges]

    
    def calling_judges(self,
                       user_request:str,
                       context_document:str,
                       test_model_response:str,
                       test_model :str,
                       skip_eval:bool,
                       sample_id : str,
                       evaluation_prompt_file_path:str,
                       tracer_project:str=None
                       ):
        """Coordinates the evaluation of model responses using the judge models.
        
        This method orchestrates the process of evaluating a language model's response
        to a user request. It handles both cases where a test model response is available
        or not available for evaluation.
        
        Args:
            user_request (str): The original query or instruction from the user.
            context_document (str): The reference document or context provided for the task.
            test_model_response (str): The response generated by the test model to be evaluated.
            test_model (str): The name or identifier of the model being evaluated.
            skip_eval (bool): Flag indicating whether a response from the test model is available.
            sample_id (str): Unique identifier for the evaluation sample.
            evaluation_prompt_file_path (str): Path to the file containing evaluation prompt templates.
            tracer_project (str, optional): Name of the LangSmith tracing project for logging. Defaults to None.
            
        Returns:
            List[Dict]: A list of dictionaries containing evaluation results from each judge model,
                including verdicts and any encountered problems.
        """
        
        judge_responses =  []
        if skip_eval:
            for j in self.judges:
                judge_response = {
                    'judge model':j.model,
                    'sample id':sample_id,
                    'judged model':test_model,
                    "Encountered Problems": "Not applicable",
                    'judge model response':"Not applicable",
                    'verdict':"Not applicable"
                }
                judge_responses.append(judge_response)
                dump_in_jsonl(judge_response,"judge_responses.jsonl")

        else : 
            for j in self.judges:

                if "anthropic" in j.model:
                    evaluation_method = "implicit_span_level"
                else : 
                    evaluation_method = "json"

                judge_template = self.fetch_evaluation_prompt(evaluation_prompt_file_path,evaluation_method)

                filled_prompt = self.fill_out_prompt(
                    judge_template,
                    user_request = user_request,
                    context_document = context_document,
                    response = test_model_response
                )

                judge_response = j.call_llm(
                    filled_prompt,
                    intention= "judge",
                    judged_model =  test_model,
                    sample_id = sample_id,
                    tracer_project = tracer_project
                )
                
                verdict = self.get_verdict(
                    judge_response["judge model response"],
                    evaluation_method
                    )
                
                judge_response['verdict'] = verdict
                judge_responses.append(judge_response)
                dump_in_jsonl(judge_response,"judge_responses.jsonl")

        return judge_responses
            


    def fetch_evaluation_prompt(self,evaluation_prompt_file_path:str,evaluation_method):
        """
        Fetch the evaluation prompt based on the specified evaluation method.
        
        Args:
            evaluation_prompt_file_path: Path to the CSV file containing evaluation prompts
            evaluation_method: The evaluation method to use (e.g., 'json', 'implicit_span_level')
            
        Returns:
            The evaluation prompt template string for the specified method
        """
        evaluation_prompt_file = pd.read_csv(evaluation_prompt_file_path)
        evaluation_prompt = evaluation_prompt_file.loc[evaluation_prompt_file["evaluation_method"] == evaluation_method, "evaluation_prompt"].values[0]
        return evaluation_prompt

    def fill_out_prompt(self,prompt_template: str, **kwargs) -> str:
        """
        Format a prompt template with {{variable}} style placeholders.
        
        Args:
            prompt_template: Template string with {{variable}} placeholders
            **kwargs: Variables to substitute into the template
            
        Returns:
            Formatted prompt string
        """
        try:
            for key, value in kwargs.items():
                placeholder = f"{{{{{key}}}}}"
                prompt_template = prompt_template.replace(placeholder, str(value))
            return prompt_template
        except Exception as e:
            logger.error(f"Error formatting prompt template: {e}")
            raise


    def get_verdict(self,raw_output:str,evaluation_method):
        """
        Extract the verdict from the judge model's raw output based on the evaluation method.
        
        Args:
            raw_output: The raw text response from the judge model
            evaluation_method: The evaluation method used (e.g., 'json', 'implicit_span_level')
            
        Returns:
            A verdict string (e.g., 'Accurate', 'Inaccurate') or None if parsing fails
        """
        try : 
            if evaluation_method == 'json':
                processed_output = raw_output.strip("```json").strip()
                breakdown_list = [json.loads(line) for line in processed_output.strip().split("\n")]
                labels = [i['label'] for i in breakdown_list]

                if not any(label in ["unsupported","contradictory"] for label in labels):
                    verdict = "Accurate"
                else : 
                    verdict = "Inaccurate"
            else :
                match = re.search(r'Final Answer:\s*(Accurate|Inaccurate)', raw_output)
                if match:
                    verdict = match.group(1)  # Return just the "Accurate" or "Inaccurate" part
                else:
                    verdict = None

            logger.info(f"{verdict}")
            return verdict
        
        except Exception as e:
            logger.error(f"Error in get_verdict : {e}")
            verdict = None





